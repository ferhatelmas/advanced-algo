Our recursive \textit{dynamic programming} algorithm:
\begin{itemize}
  \item Divide the sequence of matrices into two sequences
  \item Find the cost of the multiplication of each sequence.  
  \item If the cost of the multiplication is in the table, then simply return it. Otherwise, recursively calculate and add final cost into the cost table. Here, base case is  the multiplication of two matrices. When base is arrived, we just return the cost: If we have two $M_1$ is a $k \times l$ matrix and $M_2$ is a  $l \times m$ matrix, then the cost of the multiplication $C$ is $klm$. For example, $M_1(20 \times 30)$ and $M_2(30 \times 10)$, cost $C$ is $20 \cdot 30 \cdot 10 = 6000$.
  \item Sum up the costs of the sub-sequences and record it as a candidate for the optimal cost of the original sequence.
  \item Repeat above three steps for original sequence where it can be divided into two sequences and take the minimum of all calculated candidate costs and store it into the cost table for the cost of the original sequence.
\end{itemize}

A sequence is defined by its \textit{beginning} and \textit{ending} indices. Hence, in the cost table, we need $O(n^2)$ space since the number of sequences are defined as in the following:
$$
  \sum_{i=1}^{n-1}{\sum_{j=i}^{n-1} 1} \leq O(n^2)
$$

As a result of this number of sequences, we need at least $O(n^2)$ sub-sequence computations. However, we also go over the sequence to find the minimum of each possible sub-sequence so each entry of the table requires us to scan the sequence which takes $O(n)$ time. That's why defined \textit{top-down dynamic programming} algorithm takes $O(n^3)$ time at overall which is a big improvement over intuitive \textit{brute-force} $O(2^n)$ algorithm.